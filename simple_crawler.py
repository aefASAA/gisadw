#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆç½‘ç»œå…³é”®è¯çˆ¬è™«
ä¸“é—¨ç”¨äºŽå¿«é€Ÿæœç´¢å’ŒèŽ·å–ç»“æžœ
"""

import requests
import time
import random
from urllib.parse import quote
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import json

class SimpleCrawler:
    """ç®€åŒ–ç‰ˆçˆ¬è™«ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        self.results = []
    
    def search_baidu(self, keyword, max_pages=3):
        """ç™¾åº¦æœç´¢"""
        print(f"æ­£åœ¨æœç´¢ç™¾åº¦: {keyword}")
        results = []
        
        for page in range(max_pages):
            try:
                pn = page * 10
                url = f"https://www.baidu.com/s?wd={quote(keyword)}&pn={pn}"
                
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æœç´¢ç»“æžœ
                search_results = soup.find_all('div', class_='result')
                
                for result in search_results:
                    try:
                        # èŽ·å–æ ‡é¢˜
                        title_elem = result.find('h3')
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        link = title_elem.find('a')['href'] if title_elem.find('a') else ''
                        
                        # èŽ·å–æ‘˜è¦
                        abstract_elem = result.find('div', class_='c-abstract')
                        abstract = abstract_elem.get_text(strip=True) if abstract_elem else ''
                        
                        # èŽ·å–æ¥æº
                        source_elem = result.find('div', class_='c-abstract-source')
                        source = source_elem.get_text(strip=True) if source_elem else ''
                        
                        results.append({
                            'title': title,
                            'link': link,
                            'abstract': abstract,
                            'source': source,
                            'search_engine': 'ç™¾åº¦',
                            'keyword': keyword,
                            'page': page + 1
                        })
                        
                    except Exception as e:
                        continue
                
                print(f"ç¬¬ {page + 1} é¡µå®Œæˆï¼ŒèŽ·å– {len(search_results)} ä¸ªç»“æžœ")
                time.sleep(random.uniform(1, 2))  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                print(f"ç™¾åº¦æœç´¢ç¬¬ {page + 1} é¡µå¤±è´¥: {e}")
                continue
        
        return results
    
    def search_bing(self, keyword, max_pages=3):
        """å¿…åº”æœç´¢"""
        print(f"æ­£åœ¨æœç´¢å¿…åº”: {keyword}")
        results = []
        
        for page in range(max_pages):
            try:
                first = page * 10
                url = f"https://www.bing.com/search?q={quote(keyword)}&first={first}"
                
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æœç´¢ç»“æžœ
                search_results = soup.find_all('li', class_='b_algo')
                
                for result in search_results:
                    try:
                        # èŽ·å–æ ‡é¢˜
                        title_elem = result.find('h3')
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        link = title_elem.find('a')['href'] if title_elem.find('a') else ''
                        
                        # èŽ·å–æ‘˜è¦
                        abstract_elem = result.find('p')
                        abstract = abstract_elem.get_text(strip=True) if abstract_elem else ''
                        
                        # èŽ·å–æ¥æº
                        source_elem = result.find('cite')
                        source = source_elem.get_text(strip=True) if source_elem else ''
                        
                        results.append({
                            'title': title,
                            'link': link,
                            'abstract': abstract,
                            'source': source,
                            'search_engine': 'å¿…åº”',
                            'keyword': keyword,
                            'page': page + 1
                        })
                        
                    except Exception as e:
                        continue
                
                print(f"ç¬¬ {page + 1} é¡µå®Œæˆï¼ŒèŽ·å– {len(search_results)} ä¸ªç»“æžœ")
                time.sleep(random.uniform(1, 2))
                
            except Exception as e:
                print(f"å¿…åº”æœç´¢ç¬¬ {page + 1} é¡µå¤±è´¥: {e}")
                continue
        
        return results
    
    def search_all(self, keyword, max_pages=3):
        """æœç´¢æ‰€æœ‰æ”¯æŒçš„æœç´¢å¼•æ“Ž"""
        all_results = []
        
        # æœç´¢ç™¾åº¦
        baidu_results = self.search_baidu(keyword, max_pages)
        all_results.extend(baidu_results)
        
        # æœç´¢å¿…åº”
        bing_results = self.search_bing(keyword, max_pages)
        all_results.extend(bing_results)
        
        return all_results
    
    def search_website(self, keyword, website_url, max_pages=3):
        """ç›´æŽ¥çˆ¬å–æŒ‡å®šç½‘ç«™"""
        print(f"æ­£åœ¨çˆ¬å–ç½‘ç«™: {website_url}")
        print(f"æœç´¢å…³é”®è¯: {keyword}")
        results = []
        
        try:
            # æ›´æ–°è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®žæµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
                'Referer': 'https://www.google.com/',
                'DNT': '1',
                'Sec-Fetch-User': '?1'
            }
            
            print(f"ðŸ” æ­£åœ¨å‘é€è¯·æ±‚åˆ°: {website_url}")
            
            # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´å’Œé‡è¯•æœºåˆ¶
            for attempt in range(3):
                try:
                    response = self.session.get(website_url, headers=headers, timeout=30)
                    response.raise_for_status()
                    print(f"âœ… è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
                    print(f"ðŸ“„ å“åº”å¤§å°: {len(response.text)} å­—ç¬¦")
                    print(f"ðŸ” å“åº”ç¼–ç : {response.encoding}")
                    print(f"ðŸ” å“åº”å¤´Content-Type: {response.headers.get('Content-Type', 'æœªçŸ¥')}")
                    break
                except Exception as e:
                    if attempt == 2:
                        raise e
                    print(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•...")
                    time.sleep(3)
            
            # å°è¯•å¤šç§ç¼–ç æ–¹å¼
            html_content = None
            encodings_to_try = ['utf-8', 'gbk', 'gb2312', 'big5', 'latin1']
            
            for encoding in encodings_to_try:
                try:
                    response.encoding = encoding
                    html_content = response.text
                    print(f"âœ… ä½¿ç”¨ç¼–ç  {encoding} æˆåŠŸè§£æž")
                    break
                except Exception as e:
                    print(f"âŒ ç¼–ç  {encoding} å¤±è´¥: {e}")
                    continue
            
            if html_content is None:
                # å¦‚æžœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ–¹å¼
                html_content = response.text
                print("âš ï¸ ä½¿ç”¨é»˜è®¤ç¼–ç è§£æž")
            
            # å°è¯•å¤šç§è§£æžå™¨
            soup = None
            parsers_to_try = ['html.parser', 'lxml', 'html5lib']
            
            for parser in parsers_to_try:
                try:
                    soup = BeautifulSoup(html_content, parser)
                    print(f"âœ… ä½¿ç”¨è§£æžå™¨ {parser} æˆåŠŸ")
                    break
                except Exception as e:
                    print(f"âŒ è§£æžå™¨ {parser} å¤±è´¥: {e}")
                    continue
            
            if soup is None:
                raise Exception("æ‰€æœ‰è§£æžå™¨éƒ½å¤±è´¥äº†")
            
            # è°ƒè¯•ï¼šæ£€æŸ¥é¡µé¢åŸºæœ¬ç»“æž„
            print(f"ðŸ” é¡µé¢æ ‡é¢˜: {soup.title.get_text() if soup.title else 'æ— æ ‡é¢˜'}")
            print(f"ðŸ” æ‰¾åˆ°çš„æ ‡é¢˜æ ‡ç­¾æ•°é‡: {len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']))}")
            print(f"ðŸ” æ‰¾åˆ°çš„æ®µè½æ ‡ç­¾æ•°é‡: {len(soup.find_all('p'))}")
            print(f"ðŸ” æ‰¾åˆ°çš„é“¾æŽ¥æ ‡ç­¾æ•°é‡: {len(soup.find_all('a'))}")
            print(f"ðŸ” æ‰¾åˆ°çš„divæ ‡ç­¾æ•°é‡: {len(soup.find_all('div'))}")
            print(f"ðŸ” æ‰¾åˆ°çš„spanæ ‡ç­¾æ•°é‡: {len(soup.find_all('span'))}")
            
            # å°è¯•æŸ¥æ‰¾JavaScriptå˜é‡ä¸­çš„å†…å®¹
            print("ðŸ” æœç´¢JavaScriptå˜é‡ä¸­çš„å†…å®¹...")
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    script_content = script.string
                    if keyword.lower() in script_content.lower():
                        print(f"âœ… åœ¨JavaScriptä¸­æ‰¾åˆ°å…³é”®è¯")
                        # æå–åŒ…å«å…³é”®è¯çš„ä¸Šä¸‹æ–‡
                        keyword_index = script_content.lower().find(keyword.lower())
                        if keyword_index != -1:
                            start = max(0, keyword_index - 100)
                            end = min(len(script_content), keyword_index + 100)
                            context = script_content[start:end]
                            
                            results.append({
                                'title': f"JavaScriptä¸­çš„å…³é”®è¯å†…å®¹",
                                'link': website_url,
                                'abstract': context,
                                'source': website_url,
                                'search_engine': 'ç›´æŽ¥çˆ¬å–',
                                'keyword': keyword,
                                'page': 1
                            })
            
            # æ–¹æ³•1: æŸ¥æ‰¾æ ‡é¢˜åŒ…å«å…³é”®è¯çš„å…ƒç´ ï¼ˆæ›´å…¨é¢çš„æ ‡é¢˜æ ‡ç­¾ï¼‰
            title_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'title', 'a'])
            print(f"ðŸ” å¼€å§‹æœç´¢æ ‡é¢˜å…ƒç´ ï¼Œå…±æ‰¾åˆ° {len(title_elements)} ä¸ª")
            
            for title_elem in title_elements:
                title_text = title_elem.get_text(strip=True)
                if keyword.lower() in title_text.lower() and len(title_text) > 2:
                    print(f"âœ… åœ¨æ ‡é¢˜ä¸­æ‰¾åˆ°å…³é”®è¯: {title_text[:50]}...")
                    # æŸ¥æ‰¾ç›¸å…³çš„é“¾æŽ¥å’Œæ‘˜è¦
                    link_url = ''
                    if title_elem.name == 'a' and title_elem.get('href'):
                        link_url = title_elem['href']
                    else:
                        link = title_elem.find('a')
                        if link and link.get('href'):
                            link_url = link['href']
                    
                    # å¤„ç†ç›¸å¯¹é“¾æŽ¥
                    if link_url and not link_url.startswith('http'):
                        if link_url.startswith('/'):
                            link_url = website_url.rstrip('/') + link_url
                        else:
                            link_url = website_url.rstrip('/') + '/' + link_url
                    
                    # æŸ¥æ‰¾æ‘˜è¦ï¼ˆæ›´æ™ºèƒ½çš„æ‘˜è¦æŸ¥æ‰¾ï¼‰
                    abstract = ''
                    # æŸ¥æ‰¾æ ‡é¢˜é™„è¿‘çš„æ®µè½
                    next_elem = title_elem.find_next_sibling()
                    if next_elem and next_elem.name == 'p':
                        abstract = next_elem.get_text(strip=True)
                    # æŸ¥æ‰¾çˆ¶å…ƒç´ ä¸­çš„æ®µè½
                    elif title_elem.parent:
                        parent_para = title_elem.parent.find('p')
                        if parent_para:
                            abstract = parent_para.get_text(strip=True)
                    
                    results.append({
                        'title': title_text,
                        'link': link_url,
                        'abstract': abstract,
                        'source': website_url,
                        'search_engine': 'ç›´æŽ¥çˆ¬å–',
                        'keyword': keyword,
                        'page': 1
                    })
            
            # æ–¹æ³•2: æŸ¥æ‰¾æ®µè½ä¸­åŒ…å«å…³é”®è¯çš„å†…å®¹ï¼ˆæ›´æ™ºèƒ½çš„æ®µè½æŸ¥æ‰¾ï¼‰
            paragraphs = soup.find_all(['p', 'div', 'span'])
            print(f"ðŸ” å¼€å§‹æœç´¢æ®µè½å…ƒç´ ï¼Œå…±æ‰¾åˆ° {len(paragraphs)} ä¸ª")
            
            for p in paragraphs:
                p_text = p.get_text(strip=True)
                if (keyword.lower() in p_text.lower() and 
                    len(p_text) > 15 and 
                    len(p_text) < 500):  # é¿å…è¿‡é•¿çš„å†…å®¹
                    
                    print(f"âœ… åœ¨æ®µè½ä¸­æ‰¾åˆ°å…³é”®è¯: {p_text[:50]}...")
                    
                    # æŸ¥æ‰¾ç›¸å…³çš„æ ‡é¢˜
                    title = ''
                    # å‘ä¸ŠæŸ¥æ‰¾æ ‡é¢˜
                    prev_elem = p.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                    if prev_elem:
                        title = prev_elem.get_text(strip=True)
                    # æŸ¥æ‰¾çˆ¶å…ƒç´ ä¸­çš„æ ‡é¢˜
                    elif p.parent:
                        parent_title = p.parent.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                        if parent_title:
                            title = parent_title.get_text(strip=True)
                    
                    # æŸ¥æ‰¾é“¾æŽ¥
                    link_url = ''
                    link = p.find('a')
                    if link and link.get('href'):
                        link_url = link['href']
                        if not link_url.startswith('http'):
                            if link_url.startswith('/'):
                                link_url = website_url.rstrip('/') + link_url
                            else:
                                link_url = website_url.rstrip('/') + '/' + link_url
                    
                    results.append({
                        'title': title or f"åŒ…å«å…³é”®è¯çš„æ®µè½",
                        'link': link_url,
                        'abstract': p_text[:200] + '...' if len(p_text) > 200 else p_text,
                        'source': website_url,
                        'search_engine': 'ç›´æŽ¥çˆ¬å–',
                        'keyword': keyword,
                        'page': 1
                    })
            
            # æ–¹æ³•3: æŸ¥æ‰¾é“¾æŽ¥æ–‡æœ¬åŒ…å«å…³é”®è¯çš„é“¾æŽ¥
            links = soup.find_all('a')
            print(f"ðŸ” å¼€å§‹æœç´¢é“¾æŽ¥å…ƒç´ ï¼Œå…±æ‰¾åˆ° {len(links)} ä¸ª")
            
            for link in links:
                link_text = link.get_text(strip=True)
                if (keyword.lower() in link_text.lower() and 
                    len(link_text) > 2 and 
                    len(link_text) < 100):  # é¿å…è¿‡é•¿çš„é“¾æŽ¥æ–‡æœ¬
                    
                    print(f"âœ… åœ¨é“¾æŽ¥ä¸­æ‰¾åˆ°å…³é”®è¯: {link_text[:50]}...")
                    
                    link_url = link.get('href', '')
                    if link_url and not link_url.startswith('http'):
                        if link_url.startswith('/'):
                            link_url = website_url.rstrip('/') + link_url
                        else:
                            link_url = website_url.rstrip('/') + '/' + link_url
                    
                    # æŸ¥æ‰¾é“¾æŽ¥é™„è¿‘çš„æ‘˜è¦
                    abstract = ''
                    next_elem = link.find_next_sibling()
                    if next_elem and next_elem.name == 'p':
                        abstract = next_elem.get_text(strip=True)
                    # æŸ¥æ‰¾çˆ¶å…ƒç´ ä¸­çš„æ®µè½
                    elif link.parent:
                        parent_para = link.parent.find('p')
                        if parent_para:
                            abstract = parent_para.get_text(strip=True)
                    
                    results.append({
                        'title': link_text,
                        'link': link_url,
                        'abstract': abstract,
                        'source': website_url,
                        'search_engine': 'ç›´æŽ¥çˆ¬å–',
                        'keyword': keyword,
                        'page': 1
                    })
            
            # æ–¹æ³•4: æŸ¥æ‰¾è¡¨æ ¼ä¸­åŒ…å«å…³é”®è¯çš„å†…å®¹
            tables = soup.find_all('table')
            print(f"ðŸ” å¼€å§‹æœç´¢è¡¨æ ¼å…ƒç´ ï¼Œå…±æ‰¾åˆ° {len(tables)} ä¸ª")
            
            for table in tables:
                table_text = table.get_text(strip=True)
                if keyword.lower() in table_text.lower():
                    print(f"âœ… åœ¨è¡¨æ ¼ä¸­æ‰¾åˆ°å…³é”®è¯")
                    # æŸ¥æ‰¾è¡¨æ ¼æ ‡é¢˜
                    caption = table.find('caption')
                    title = caption.get_text(strip=True) if caption else "åŒ…å«å…³é”®è¯çš„è¡¨æ ¼"
                    
                    # æå–è¡¨æ ¼æ‘˜è¦
                    rows = table.find_all('tr')
                    table_summary = []
                    for row in rows[:3]:  # åªå–å‰3è¡Œä½œä¸ºæ‘˜è¦
                        cells = row.find_all(['td', 'th'])
                        row_text = ' | '.join([cell.get_text(strip=True) for cell in cells])
                        if row_text:
                            table_summary.append(row_text)
                    
                    abstract = ' | '.join(table_summary)
                    
                    results.append({
                        'title': title,
                        'link': '',
                        'abstract': abstract,
                        'source': website_url,
                        'search_engine': 'ç›´æŽ¥çˆ¬å–',
                        'keyword': keyword,
                        'page': 1
                    })
            
            print(f"ðŸ” åˆæ­¥æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æžœ")
            
            # åŽ»é‡ç»“æžœï¼ˆåŸºäºŽæ ‡é¢˜å’Œæ‘˜è¦çš„ç»„åˆï¼‰
            unique_results = []
            seen_content = set()
            for result in results:
                content_key = f"{result['title']}_{result['abstract'][:50]}"
                if content_key not in seen_content:
                    unique_results.append(result)
                    seen_content.add(content_key)
            
            print(f"ðŸ” åŽ»é‡åŽå‰©ä½™ {len(unique_results)} ä¸ªç»“æžœ")
            
            # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°ç»“æžœï¼Œå°è¯•æ›´å®½æ¾çš„æœç´¢
            if not unique_results:
                print("âš ï¸ æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³Šæœç´¢...")
                # æœç´¢åŒ…å«å…³é”®è¯å­—ç¬¦çš„å†…å®¹
                all_text = soup.get_text()
                print(f"ðŸ” é¡µé¢æ€»æ–‡æœ¬é•¿åº¦: {len(all_text)} å­—ç¬¦")
                
                if keyword.lower() in all_text.lower():
                    print("âœ… é¡µé¢ç¡®å®žåŒ…å«å…³é”®è¯ï¼Œå°è¯•æå–ä¸Šä¸‹æ–‡...")
                    # æå–åŒ…å«å…³é”®è¯çš„ä¸Šä¸‹æ–‡
                    keyword_index = all_text.lower().find(keyword.lower())
                    if keyword_index != -1:
                        start = max(0, keyword_index - 100)
                        end = min(len(all_text), keyword_index + 100)
                        context = all_text[start:end]
                        
                        unique_results.append({
                            'title': f"åŒ…å«å…³é”®è¯çš„é¡µé¢å†…å®¹",
                            'link': website_url,
                            'abstract': context,
                            'source': website_url,
                            'search_engine': 'ç›´æŽ¥çˆ¬å–',
                            'keyword': keyword,
                            'page': 1
                        })
                        print("âœ… é€šè¿‡æ¨¡ç³Šæœç´¢æ‰¾åˆ°ç›¸å…³å†…å®¹")
                else:
                    print("âŒ é¡µé¢ä¸­ç¡®å®žæ²¡æœ‰æ‰¾åˆ°å…³é”®è¯")
                    
                    # å°è¯•æœç´¢å…³é”®è¯çš„éƒ¨åˆ†å­—ç¬¦
                    print("ðŸ” å°è¯•æœç´¢å…³é”®è¯çš„éƒ¨åˆ†å­—ç¬¦...")
                    for i in range(len(keyword), 1, -1):
                        partial_keyword = keyword[:i]
                        if partial_keyword.lower() in all_text.lower():
                            print(f"âœ… æ‰¾åˆ°éƒ¨åˆ†å…³é”®è¯: {partial_keyword}")
                            keyword_index = all_text.lower().find(partial_keyword.lower())
                            start = max(0, keyword_index - 100)
                            end = min(len(all_text), keyword_index + 100)
                            context = all_text[start:end]
                            
                            unique_results.append({
                                'title': f"åŒ…å«éƒ¨åˆ†å…³é”®è¯'{partial_keyword}'çš„é¡µé¢å†…å®¹",
                                'link': website_url,
                                'abstract': context,
                                'source': website_url,
                                'search_engine': 'ç›´æŽ¥çˆ¬å–',
                                'keyword': keyword,
                                'page': 1
                            })
                            break
            
            print(f"ðŸŽ¯ æœ€ç»ˆç»“æžœæ•°é‡: {len(unique_results)}")
            return unique_results
            
        except Exception as e:
            print(f"âŒ çˆ¬å–ç½‘ç«™å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def save_results(self, results, filename=None, format='excel'):
        """ä¿å­˜æœç´¢ç»“æžœ"""
        if not results:
            print("æ²¡æœ‰ç»“æžœå¯ä¿å­˜")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"search_{timestamp}"
        
        try:
            df = pd.DataFrame(results)
            
            if format.lower() == 'excel':
                filepath = f"{filename}.xlsx"
                df.to_excel(filepath, index=False, engine='openpyxl')
                print(f"ç»“æžœå·²ä¿å­˜åˆ°: {filepath}")
                
            elif format.lower() == 'csv':
                filepath = f"{filename}.csv"
                df.to_csv(filepath, index=False, encoding='utf-8-sig')
                print(f"ç»“æžœå·²ä¿å­˜åˆ°: {filepath}")
                
            elif format.lower() == 'json':
                filepath = f"{filename}.json"
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"ç»“æžœå·²ä¿å­˜åˆ°: {filepath}")
                
            else:
                print(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {format}")
                
        except Exception as e:
            print(f"ä¿å­˜ç»“æžœå¤±è´¥: {e}")
    
    def print_summary(self, results):
        """æ‰“å°ç»“æžœæ‘˜è¦"""
        if not results:
            print("æ²¡æœ‰æœç´¢ç»“æžœ")
            return
        
        print(f"\n=== æœç´¢ç»“æžœæ‘˜è¦ ===")
        print(f"æ€»ç»“æžœæ•°: {len(results)}")
        
        # æŒ‰æœç´¢å¼•æ“Žç»Ÿè®¡
        engine_stats = {}
        for result in results:
            engine = result.get('search_engine', 'æœªçŸ¥')
            engine_stats[engine] = engine_stats.get(engine, 0) + 1
        
        print(f"\næŒ‰æœç´¢å¼•æ“Žç»Ÿè®¡:")
        for engine, count in engine_stats.items():
            print(f"  {engine}: {count} ä¸ªç»“æžœ")
        
        # æ˜¾ç¤ºå‰3ä¸ªç»“æžœ
        print(f"\nå‰3ä¸ªç»“æžœé¢„è§ˆ:")
        for i, result in enumerate(results[:3], 1):
            print(f"\n{i}. {result.get('title', 'æ— æ ‡é¢˜')}")
            print(f"   æ¥æº: {result.get('source', 'æœªçŸ¥')}")
            print(f"   æœç´¢å¼•æ“Ž: {result.get('search_engine', 'æœªçŸ¥')}")
            if result.get('abstract'):
                print(f"   æ‘˜è¦: {result.get('abstract', '')[:80]}...")


def main():
    """ä¸»å‡½æ•°"""
    print("=== ç®€åŒ–ç‰ˆç½‘ç»œå…³é”®è¯çˆ¬è™« ===")
    
    try:
        # èŽ·å–ç”¨æˆ·è¾“å…¥
        keyword = input("è¯·è¾“å…¥è¦æœç´¢çš„å…³é”®è¯: ").strip()
        if not keyword:
            print("å…³é”®è¯ä¸èƒ½ä¸ºç©º")
            return
        
        max_pages = input("è¯·è¾“å…¥è¦æœç´¢çš„é¡µæ•° (é»˜è®¤2é¡µ): ").strip()
        max_pages = int(max_pages) if max_pages.isdigit() else 2
        
        # åˆ›å»ºçˆ¬è™«å®žä¾‹
        print(f"\næ­£åœ¨æœç´¢å…³é”®è¯: {keyword}")
        crawler = SimpleCrawler()
        
        # å¼€å§‹æœç´¢
        results = crawler.search_all(keyword, max_pages)
        
        # æ˜¾ç¤ºç»“æžœæ‘˜è¦
        crawler.print_summary(results)
        
        # ä¿å­˜ç»“æžœ
        if results:
            save_choice = input(f"\næ˜¯å¦ä¿å­˜ç»“æžœ? (y/n, é»˜è®¤y): ").strip().lower()
            if save_choice != 'n':
                format_choice = input("é€‰æ‹©ä¿å­˜æ ¼å¼ (excel/csv/json, é»˜è®¤excel): ").strip().lower()
                format_choice = format_choice if format_choice in ['excel', 'csv', 'json'] else 'excel'
                
                filename = f"search_{keyword}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                crawler.save_results(results, filename, format_choice)
        
        print(f"\næœç´¢å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nç¨‹åºè¿è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    main()
